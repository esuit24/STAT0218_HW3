---
title: "Stat_Hw3"
author: "Ellie Suit, Andy Atallah, and Ai Hattori"
date: "2023-11-2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Research question

Research Question: 
Analyze vehicleSearch before and after George Floyd incident
- Use race, preRace, gender, policePrecinct, reason, problem, lat, long as a predictor variable 

Types of Trees: 
- Decision/Regression Tree (rpart) - simplist model 
- Bagged Trees (ipred) - making a bunch of similar trees with different observations and determining oob 
- Random Forest (randomforest) - making a bunch of weak learners and taking majority vote
- Boosting Trees (xgboost) - making one tree model and predicting the error for a given variable X 


Method: 
Predict vehicleSearch by using race, preRace, gender, case, policePrecinct, reason, problem, lat, long, and night (whether a given incident happened between 7 pm and 6 am) as a predictor variable 

## load libraries and data
```{r}
# import libraries
library(lubridate)
library(sf)
library(tidyverse)
library(rattle)
library(rpart)
library(ipred) # for bagging
library(caret) # for confusion matrix
library(randomForest) # for random forest and bagging

# set a seed
set.seed(1)

# read in csv
stop_data <- read_csv("Minneapolis_Police_Stop_Data.csv")
```

## Data preparation
```{r}
stop_data_factored <- stop_data %>%
  na.omit() %>%
  mutate(time = as_datetime(responseDate),
         reason = factor(reason),
         problem = factor(problem),
         citationIssued = factor(citationIssued),
         personSearch = factor(personSearch),
         vehicleSearch = factor(vehicleSearch),
         preRace = factor(preRace),
         race = factor(race),
         gender = factor(gender),
         policePrecinct = factor(policePrecinct))

stop_data_factored <- stop_data_factored %>%
  mutate(night = case_when(hour(time) >= 19 ~ 1,
                           hour(time) < 6 ~ 1,
                           TRUE ~ 0)) %>%
  mutate(night = factor(night))

gf_date <- as.Date("2020-05-26")
stop_data_factored <- stop_data_factored %>%
  mutate(case = case_when(time <= gf_date ~ 0,
                          time > gf_date ~ 1)) %>%
  mutate(case = factor(case))
```


# 2. Answer research questions by random forest

## Random Forest

```{r}
library(randomForest)

# The original one I showed during last session (8 predictors)
# rf_vs <- randomForest(factor(vehicleSearch) ~ preRace + race + gender + case + policePrecinct + night + reason + problem, data = stop_data_factored, cutoff = c(0.999999, 0.000001), ntree = 500, importance = TRUE)
# rf_vs

# new tree 
rf_vs2 <- randomForest(factor(vehicleSearch) ~ preRace + race + gender + case + policePrecinct + night + reason + problem + lat + long, data = stop_data_factored, ntree = 500, cutoff = c(0.6, 0.4), importance = TRUE)
# 0.31315 G-Mean
rf_vs2



rf_vs3 <- randomForest(factor(vehicleSearch) ~ preRace + race + gender + case + policePrecinct + night + reason + problem + lat + long, data = stop_data_factored, cutoff = c(0.9, 0.1), ntree = 500, importance = TRUE)
rf_vs3
#0.57299 g-mean 
```

## variable importance (this needs to be changed)
```{r}
rf_vs
rf_vs$mtry
rf_vs$importance
table(stop_data_factored$race, stop_data_factored$vehicleSearch)
```

# 3. Compare three tree-based methods (a single tree, random forest, and bagging)

## Single classification tree
```{r}
rows_to_keep <- sample(1:nrow(stop_data_factored),
                       size = nrow(stop_data_factored)/2)
train <- stop_data_factored[rows_to_keep,]
test <- stop_data_factored[-rows_to_keep,]
tree1 <- rpart(factor(vehicleSearch) ~ race + gender + policePrecinct + reason + problem + case + lat + long + preRace + night, 
               control = rpart.control(cp=0.00098),
              data = train)
test_input <- test %>%
  select(race, gender, policePrecinct, reason, problem, case, lat, long, preRace, night)
preds <- predict(tree1, test_input)
#TODO: why does this not work for a binary variable vehicleSearch? 
fancyRpartPlot(tree1,
               cex = .4)
preds <- as.data.frame(preds)

preds_clean <- preds %>%
  mutate(v_s = case_when(YES < 0.5 ~ 0,
                         TRUE ~ 1)) 
tab <- table(preds_clean$v_s, test$vehicleSearch)
tab 

```


## Bagging

Useful link: https://bookdown.org/tpinto_home/Beyond-Additivity/bagging.html 

### Build a bagged model and find the overall OOB error
```{r}
library(randomForest)
set.seed(1)

p <- 10  # number of predictor variables

bag_veh_rf <- randomForest(factor(vehicleSearch) ~ reason + problem +
                    + preRace + race + gender + lat + long + policePrecinct + case + night,
                data = stop_data_factored,
                mtry = p, # bagging is a special case of a random forest where m = p
                importance = TRUE) 

bag_veh_rf
```

### Measure variable importance

Gini index: "Gini index is referred to as a measure of
node purityâ€”a small value indicates that a node contains predominantly
observations from a single class." (p.336 of the text, p.344 for PDF)

Mean decrease accuracy: mean difference between the classification accuracy of whole trees and that of trees that did not see the predictor variable over all classes ("YES" and "NO" in this case) (https://www.rdocumentation.org/packages/randomForest/versions/4.7-1.1/topics/randomForest)

```{r}
bag_veh_rf$importance

# visualize
varImpPlot(bag_veh_rf,
           main = "Variable importance for bagging")
```

Answers to our research questions:
1) Is whether a traffic stop occurred before or after the murder of George Floyd an important factor in whether the stop progressed to a vehicle search?

In the above figure for variable importance, case has the second smallest mean decrease accuracy and the fifth smallest mean decrease Gini. Since the higher the Mean Decrease Accuracy and the Mean Decrease Gini are the more important the variable is, this suggests that case (whether a traffic stop occurred before or after the murder of George Floyd) is not an important factor in whether the stop progressed to a vehicle search.

2) Which other factors can predict whether a stop progressed to a vehicle search? Do they suggest police bias?

The above figure for variable importance show that problem, reason, and race are top three factors that increase the classification accuracy of the model the most. It also indicates that long, lat, and race are top three factors that contribute to increasing the Gini index (node heterogeneity) the most. The fact that race is in the top three for both mean decrease accuracy and mean decrease Gini indicates that it is important in predicting whether a stop progressed to a vehicle search. This means that people of certain races are more likely to get vehicle searched at a traffic stop, which implies the existence of a racial bias of police officers in Minneapolis.

### Find predictor variable with the smallest mean decrease accuracy and predictor variable with the smallest mean decrease Gini

```{r}
bag_var_imp <- as.data.frame(bag_veh_rf$importance) # create a dataframe just for importance
index_mda <- which.min(bag_var_imp[,3]) # find an index for a predictor with the smallest mean decrease accuracy
index_mdg <- which.min(bag_var_imp[,4]) # find an index for a predictor with the smallest mean decrease Gini
rownames(bag_var_imp)[index_mda] # get a name for a predictor with the smallest mean decrease accuracy
rownames(bag_var_imp)[index_mdg] # get a name for a predictor with the smallest mean decrease Gini
```

### Remove the least important predictor variables found above, build a new bagged model and find the overall OOB error
```{r}
library(randomForest)
set.seed(1)

p <- 8  # number of predictor variables

bag_veh_rf2 <- randomForest(factor(vehicleSearch) ~ reason + problem +
                    + preRace + race + gender + lat + long + case,
                data = stop_data_factored,
                mtry = p, # bagging is a special case of a random forest where m = p
                importance = TRUE) 

bag_veh_rf2
```

# scraped work 

### Data preparation
```{r}

# before_gf <- stop_data_factored %>%
#   filter(case == 0) 
# after_gf <- stop_data_factored %>%
#   filter(case == 1)

# table(stop_data_factored$race)
```

### random forest

# remove less important pred vars
```{r}
set.seed(1)
rf_vs_2 <- randomForest(factor(vehicleSearch) ~ preRace + race + gender + policePrecinct + reason + problem, data = stop_data_factored, cutoff = c(0.999999, 0.000001), ntree = 500, importance = TRUE)

rf_vs_2
rf_vs_2$mtry
rf_vs_2$importance
table(stop_data_factored$race, stop_data_factored$vehicleSearch)
```

# include case as a predictor variable
```{r}
set.seed(1)
rf_vs_3 <- randomForest(factor(vehicleSearch) ~ preRace + race + gender + policePrecinct + reason + problem + case, data = stop_data_factored, cutoff = c(0.999999, 0.000001), ntree = 500, importance = TRUE)

rf_vs_3
rf_vs_3$mtry
rf_vs_3$importance
table(stop_data_factored$race, stop_data_factored$vehicleSearch)
```


# include night as a predictor variable
```{r}
set.seed(1)
rf_vs_4 <- randomForest(factor(vehicleSearch) ~ preRace + race + gender + policePrecinct + reason + problem + night, data = stop_data_factored, cutoff = c(0.999999, 0.000001), ntree = 500, importance = TRUE)

rf_vs_4
rf_vs_4$mtry
rf_vs_4$importance
table(stop_data_factored$race, stop_data_factored$vehicleSearch)
```

```{r}
library(MASS)
rows_to_keep <- sample(1:nrow(stop_data_factored),
                size = nrow(stop_data_factored)/2)
train <- stop_data_factored[rows_to_keep,]
test <- stop_data_factored[-rows_to_keep,] 

lda_model <- lda(factor(vehicleSearch) ~ preRace + race + gender + policePrecinct + reason + problem, data = train)
preds <- predict(lda_model, test)

table(preds$class, test$vehicleSearch)
```


```{r}
library(randomForest)
rf1 <- randomForest(preRace ~ reason + problem + gender + policePrecinct + night + lat + long, data = stop_data_factored, mtry = 4, importance = TRUE) 

rf2 <- randomForest(race ~ reason + problem  + personSearch + gender + policePrecinct + lat + long, data = stop_data_factored, mtry = 4, importance = TRUE)


view(rf1$importance)

view(rf2$importance) 

rf1
rf2
```


### Boosting

```{r}
library(xgboost)

# Predict vehicleSearch
# Wants a matrix input
set.seed(1)
train_rows <- sample(1:nrow(stop_data_factored), size=nrow(stop_data_factored)/2)

train <- stop_data_factored[train_rows,]
test <- stop_data_factored[-train_rows,]

train$problem <- factor(as.numeric(train$problem))
train$reason <- factor(as.numeric(train$reason))
train$gender <- factor(as.numeric(train$gender))
train$race <- factor(as.numeric(train$race))
train$preRace <- factor(as.numeric(train$preRace))

test$problem <- factor(as.numeric(test$problem))
test$reason <- factor(as.numeric(test$reason))
test$gender <- factor(as.numeric(test$gender))
test$race <- factor(as.numeric(test$race))
test$preRace <- factor(as.numeric(test$preRace))

train$vehicleSearch <- factor(as.numeric(train$vehicleSearch))
test$vehicleSearch <- factor(as.numeric(test$vehicleSearch))


train$gender = as.numeric(factor(train$gender,
                                 levels=c("Male", "Female", "Gender Non-Conforming", "Unknown"),
                                 labels=c(1, 2, 3, 4)))

train$vehicleSearch = as.numeric(factor(train$vehicleSearch,
                                 levels=c("NO", "YES"),
                                 labels=c(1, 2)))

test$gender = as.numeric(factor(test$gender,
                                 levels=c("Male", "Female", "Gender Non-Conforming", "Unknown"),
                                 labels=c(1, 2, 3, 4)))

test$vehicleSearch = as.numeric(factor(test$vehicleSearch,
                                 levels=c("NO", "YES"),
                                 labels=c(1, 2)))

train$vehicleSearch <- train$vehicleSearch - 1
test$vehicleSearch <- test$vehicleSearch - 1

boost1 <- xgboost(data = train %>% select(gender) %>% as.matrix(),
                  label = train$vehicleSearch,
                  nrounds=100, eval_metric = "error@0.05", objective="binary:logistic")

# by default, it will be a depth of 6 (harder to overfit)

# Predict on test
preds <- predict(boost1, test %>% select(gender) %>% as.matrix())
preds_train <- predict(boost1, train %>% select(gender) %>% as.matrix())

y_pred <- preds
y_pred_2 <- preds_train 

table(y_pred, test$vehicleSearch)
table(y_pred_2, train$vehicleSearch)


importance <- xgb.importance(model = boost1)
boost1$importance

library(caret)

caret_imp <- varImp(boost1)

caret_imp

# Optional arguments for xgboost: eval_metric for different from rmse
```




